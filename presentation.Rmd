---
title: "Comparison of Different Homogeneity Tests via Simulation Study"
subtitle: "Contrast Test, Cochran's Test, and Breslow Test"
author: "Jiaqi Bi"
institute: "Department of Epidemiology and Biostatistics, Western University"
date: "(updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader: 
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false

---

```{r setup, include=FALSE}
options(htmltools.dir.version = TRUE)
```

class: center, middle

# Homogeneity Test

### Introduction

---
class: inverse, center, middle

# Why do we need Homogeneity Test?

---

# Hypothesis Testing

Through the hypothesis, we know we are testing the measures of association $\{\theta_k\}$ on some scale $\theta_k=G(\pi_{1k},\pi_{2k})$ such that the null hypothesis is

$$
H_0:\theta_1=\theta_2=...=\theta_K
$$

--

and the alternative is

$$
\exists \theta_l\neq\theta_k\ s.t.\ 1\leq k<l\leq K
$$

--

Note that in this case, the alternative hypothesis is just a negation of the null hypothesis.  

--

In simple word, we are testing a measure **equivalence** through exposure and disease at each level. 

---

class: center, middle

# Homogeneity Test

### Methods

---

class: inverse, center, middle

# How to conduct Homogeneity Test using R? 

# How many methods are there?

---

class: inverse, center, middle

# How to conduct Homogeneity Test using R?

# ~~How many methods are there?~~

---

## Method 1: Contrast Test
The test statistic: 

$$
X^2=\Big(\mathbf{C'}\boldsymbol{\hat{\theta}}\Big)'\Big(\mathbf{C'}\boldsymbol{\hat{\Sigma}_{\boldsymbol{\hat{\theta}}}}\mathbf{C}\Big)^{-1}\mathbf{C'}\boldsymbol{\hat{\theta}} 
$$

--

**WARNING**: This does not equal to 

$$
X^2=\mathbf{C'}\boldsymbol{\hat{\theta}} \Big(\mathbf{C'}\boldsymbol{\hat{\Sigma}_{\boldsymbol{\hat{\theta}}}}\mathbf{C}\Big)^{-1}\Big(\mathbf{C'}\boldsymbol{\hat{\theta}}\Big)'
$$

---

## Method 1: Contrast Test
The test statistic: 

$$
X^2=\Big(\mathbf{C'}\boldsymbol{\hat{\theta}}\Big)'\Big(\mathbf{C'}\boldsymbol{\hat{\Sigma}_{\boldsymbol{\hat{\theta}}}}\mathbf{C}\Big)^{-1}\mathbf{C'}\boldsymbol{\hat{\theta}} 
$$
$\mathbf{C}$ is the $K\times(K-1)$ contrast matrix that it has two equivalent versions:

$$
\mathbf{C'}=
\begin{pmatrix}
1 & -1 & 0 & \dots & 0 & 0\\
0 & 1 & -1 & \dots & 0 & 0\\
\vdots & \vdots &\vdots & \vdots & \vdots & \vdots\\
0 & 0 & 0 & \dots & 1 & -1
\end{pmatrix}
$$
and
$$
\mathbf{C'}=\frac{1}{K}
\begin{pmatrix}
K-1 & -1 & \dots & -1 & -1\\
-1 & K-1 & \dots & -1 & -1 \\
\vdots & \vdots &\vdots & \vdots & \vdots \\
-1 & -1 & \dots & K-1 & -1 
\end{pmatrix}
$$
---

## Method 1: Contrast Test
The test statistic: 

$$
X^2=\Big(\mathbf{C'}\boldsymbol{\hat{\theta}}\Big)'\Big(\mathbf{C'}\boldsymbol{\hat{\Sigma}_{\boldsymbol{\hat{\theta}}}}\mathbf{C}\Big)^{-1}\mathbf{C'}\boldsymbol{\hat{\theta}} 
$$

The parameter matrix $\boldsymbol{\hat{\theta}}\stackrel{d}{\approx}N(\boldsymbol{\theta},\boldsymbol{\Sigma}_{\boldsymbol{\hat{\theta}}})$, 
$$\boldsymbol{\theta}=(\theta_1...\theta_K)^\top$$

--

and the covariance matrix $\boldsymbol{\Sigma}_{\boldsymbol{\hat{\theta}}}=V(\boldsymbol{\hat{\theta}})$ is defined as
$$\boldsymbol{\Sigma}_{\boldsymbol{\hat{\theta}}}=diag(\sigma_{\hat{\theta}_1}^2...\sigma_{\hat{\theta}_K}^2)$$
---

## Method 1: Contrast Test
The test statistic: 

$$
X^2=\Big(\mathbf{C'}\boldsymbol{\hat{\theta}}\Big)'\Big(\mathbf{C'}\boldsymbol{\hat{\Sigma}_{\boldsymbol{\hat{\theta}}}}\mathbf{C}\Big)^{-1}\mathbf{C'}\boldsymbol{\hat{\theta}} 
$$
is asymptotically distributed as $\chi^2$ with degree of freedom $K-1$.

--

Example of $K=3$: 

```r
theta <- matrix(c(table1$log.or, table2$log.or, 
                  table3$log.or), nrow = 3, ncol = 1)
Sigma <- diag(c(table1$sigma.k.sq, table2$sigma.k.sq, 
                table3$sigma.k.sq))
C.T <- matrix(c(1, 0, -1, 1, 0, -1), nrow = 2, ncol = 3)
contrast.chisq <- t(C.T %*% theta) %*% 
  solve(C.T %*% Sigma %*% t(C.T)) %*% C.T %*% theta
contrast.chisq
pchisq(contrast.chisq, df = 2, lower.tail = FALSE)
```

---

## Method 2: Cochran's Test
On stratum $k$, we have
$$\hat{\theta}_k-\theta\stackrel{d}{\approx}N(0, \sigma_{\hat{\theta}_k}^2)$$
and
$$\tau_k=\sigma_{\hat{\theta}_k}^{-2}$$
Then
$$X^2=\sum_k\hat{\tau}_k(\hat{\theta}_k-\hat{\theta})^2$$
that is asymptotically distributed as $\chi^2$ with degree of fredom $K-1$.

---

## Method 2: Cochran's Test

The Cochran's Test Statistic:
$$X^2=\sum_k\hat{\tau}_k(\hat{\theta}_k-\hat{\theta})^2$$

--

Example Code on $K=3$: 
```r
tau.k <- sigma.k.sq^(-1)
theta.C <- (table1$tau.k*table1$log.or+
              table2$tau.k*table2$log.or+
              table3$tau.k*table3$log.or)/(table1$tau.k+table2$tau.k+table3$tau.k)
cochran.chisq <- table1$tau.k*((table1$log.or - theta.C)^2) + 
  table2$tau.k*((table2$log.or - theta.C)^2) + 
  table3$tau.k*((table3$log.or - theta.C)^2)
cochran.chisq
pchisq(cochran.chisq, df = 2, lower.tail = FALSE)
```

---

## Method 3: Breslow Test
MH estimates based homogeneity test where the common odds ratio is calculated through
$$\widehat{OR}_{MH}=\frac{\sum_ka_kd_k/N_k}{\sum_kb_kc_k/N_k}$$
Each expected element of $2\times 2$ table is estimated from $OR_{MH}$, e.g.
$$\hat{E}(a_k|\widehat{OR}_{MH})=\tilde{a}_k~ s.t.~ OR_k=\widehat{OR}_{MH}$$
--

Then the test statistic is
$$X^2=\sum_{k}\frac{(a_k-\tilde{a}_k)^2}{\hat{V}(a_k|\widehat{OR}_{MH})}$$
where
$$\hat{V}(a_k|\widehat{OR}_{MH})=\Big[\frac{1}{\tilde{a}_k}+\frac{1}{\tilde{b}_k}+\frac{1}{\tilde{c}_k}+\frac{1}{\tilde{d}_k}\Big]^{-1}$$

---

## Method 3: Breslow Test
MH estimates based homogeneity test where the common odds ratio is calculated through
$$\widehat{OR}_{MH}=\frac{\sum_ka_kd_k/N_k}{\sum_kb_kc_k/N_k}$$

Each expected element of $2\times 2$ table is estimated from $OR_{MH}$, e.g.
$$\hat{E}(a_k|\widehat{OR}_{MH})=\tilde{a}_k~ s.t.~ OR_k=\widehat{OR}_{MH}$$

Or equivalently:
$$X^2=\sum_{k=1}^K\Big[\frac{(a_k-\tilde{a}_k)^2}{\tilde{a}_k}+\frac{(b_k-\tilde{b}_k)^2}{\tilde{b}_k}+\frac{(c_k-\tilde{c}_k)^2}{\tilde{c}_k}+\frac{(d_k-\tilde{d}_k)^2}{\tilde{d}_k}\Big]$$

--

It also follows $\chi^2_{K-1}$. 

---

## Method 3: Breslow Test

The test statistic

$$X^2=\sum_{k=1}^K\Big[\frac{(a_k-\tilde{a}_k)^2}{\tilde{a}_k}+\frac{(b_k-\tilde{b}_k)^2}{\tilde{b}_k}+\frac{(c_k-\tilde{c}_k)^2}{\tilde{c}_k}+\frac{(d_k-\tilde{d}_k)^2}{\tilde{d}_k}\Big]$$

Luckily, we have a Breslow Test R package to conduct above computation:

```r
library(DescTools)
MH.OR = ((table1$ak * table1$dk)/N + (table2$ak * table2$dk)/N + 
           (table3$ak * table3$dk)/N)/((table1$bk * table1$ck)/N + 
              (table2$bk * table2$ck)/N + (table3$bk * table3$ck)/N)
df <- array(
  c(table1$ak, table1$bk, table1$ck, table1$dk, 
    table2$ak, table2$bk, table2$ck, table2$dk, 
    table3$ak, table3$bk, table3$ck, table3$dk), 
  dim=c(2,2,3)
)
breslow.chisq <- BreslowDayTest(df, OR = MH.OR)
```

---

class: center, middle

# Homogeneity Test

### Simulation

---

## Log Odds Ratio

The log odds ratio with random effects:
$$\hat{\theta}_k=\theta_k+\epsilon_k$$
such that regularly,
$$\pi_i(x_i,\theta)=P(y_i=1|x_i,\theta)=\frac{\exp(\alpha+x_i\beta)}{1+\exp(\alpha+x_i\beta)}$$

--

When random effects exist: 
$$\pi_i(x_i,\theta)=P(y_i=1|x_i,\theta)=\frac{\exp(\alpha+x_i\beta+\epsilon)}{1+\exp(\alpha+x_i\beta+\epsilon)}$$
and
$$\epsilon\sim N(0,\sigma^2_{\epsilon})$$

---

## Simulation

We take $x_i$ as an indicator variable to represent exposure and unexposure, we take different $\sigma^2_{\epsilon}=\{0,0.1,...,0.9\}$ to generate normal distributed random effects, and fixed $\alpha=-0.5$ and $\beta=2.5$. Compare the homogeneity test results of total $N=\{210, 420, 630\}$ under different scenarios. 


Example of $K=3$ and $N_k=70$: 

```r
K = 3
alpha = -0.5
beta = 2.5
N = 70
n1 = 35
n2 = 35
x <- rep(1, 35)
x.beta <- beta*x
```

---

## Simulation

We take $x_i$ as an indicator variable to represent exposure and unexposure, we take different $\sigma^2_{\epsilon}=\{0,0.1,...,0.9\}$ to generate normal distributed random effects, and fixed $\alpha=-0.5$ and $\beta=2.5$. Compare the homogeneity test results of total $N=\{210, 420, 630\}$ under different scenarios. 

Example of $K=3$ and $N_k=70$ (Scenario 1): 

```r
## K1, small N
set.seed(111)
error <- rnorm(n1, 0, 0.9)
p1 <- 1/(1+exp(-(alpha + x.beta + error)))
p2 <- 1/(1+exp(-(alpha + error)))
y1 <- rbinom(n = n1, size = 1, prob = p1) 
y2 <- rbinom(n = n2, size = 1, prob = p2)
table1 <- cont.table(y1, y2)
```

---

## Simulation

The contingency table can be generated through self-defined function `cont.table(x1, x2)`:

```r
cont.table <- function(x1, x2) {
  ak <- length(which(x1==1))
  bk <- length(which(x2==1))
  ck <- length(which(x1==0))
  dk <- length(which(x2==0))
  sigma.k.sq <- 1/ak + 1/bk + 1/ck + 1/dk
  OR <- (ak*dk)/(bk*ck)
  
  return(list(
    ak=ak,
    bk=bk,
    ck=ck,
    dk=dk,
    log.or=log(OR),
    sigma.k.sq=sigma.k.sq,
    tau.k=sigma.k.sq^(-1)
  ))
}
```

---

## Simulation

Different scenarios:

- Equal sized: $n_{1k}=n_{2k}$ for all $k$

- Unequal sized: e.g. $N_k=70$, but exposure arm $n_{1k}=50$, unexposure $n_{2k}=20$

- Special case: e.g. $n_{11}=n_{21}=n_{12}=n_{22}=20$, but third strata $n_{13}=n_{23}=65$

---

## Example Results

Results cannot fit in this slide, click [here](https://github.com/Jiaqi-Bi-Not-PhD/Biostats-9510-Final-Project/blob/main/results%20presentation.jpeg)

--

```{r, include=FALSE, echo=FALSE}
library(latex2exp)
se <- seq(0, 0.9, 0.1)
#SMALL
C.s <- c(0.161, 0.330, 0.369, 0.733, 0.319, 0.624, 0.590, 0.807, 0.902, 0.939)
CT.s <- c(0.161, 0.330, 0.369, 0.733, 0.319, 0.624, 0.590, 0.807, 0.902, 0.939)
BT.s <- c(0.152, 0.313, 0.383, 0.731, 0.311, 0.622, 0.588, 0.807, 0.902, 0.939)
```

.center[
```{r, echo=FALSE}
par(mfrow=c(2,1))
matplot(se, C.s, type = "p", pch=c(1,2,3), xlab = TeX("$\\sigma^2_{\\epsilon}$"), 
        ylab = TeX("Probability of Rejecting $H_0$"),
        main= TeX("Contrast Test, $N_k=70$"))
abline(lm(C.s~se))
matplot(se, BT.s, type = "p", pch=c(1,2,3), xlab = TeX("$\\sigma^2_{\\epsilon}$"), 
        ylab = TeX("Probability of Rejecting $H_0$"),
        main= TeX("Breslow Test, $N_k=70$"))
abline(lm(BT.s~se))
```
]


--

### Interesting findings

--

- Contrast Test is algebraically equivalent to Cochran's Test

--

- Larger sample size typically yields higher type I error rate within the same random effects

--

- The higher random effects increase the homogeneity rejection rate (Yea we all know that)

--

- Results on which homogeneity test is better are not done yet, will be concluded within the final paper

---

class: inverse, center, middle

# Questions? (Better not) 

# Suggestions?


